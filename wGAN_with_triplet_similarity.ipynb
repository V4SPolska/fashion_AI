{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS + look for TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3955,
     "status": "ok",
     "timestamp": 1668173701379,
     "user": {
      "displayName": "Maks Wojtas",
      "userId": "11103437561791788771"
     },
     "user_tz": 0
    },
    "id": "1iMZmEqngjHZ",
    "outputId": "d71960cd-0ffa-431b-f721-e9ed715bd5e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!\n",
      "Fri Nov 11 13:35:00 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   52C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "All devices:  []\n",
      "2.9.2\n"
     ]
    }
   ],
   "source": [
    "#@title IMPORTS+ nvidia-smi\n",
    "from tensorflow.keras import preprocessing\n",
    "import os\n",
    "import json\n",
    "from io import BytesIO\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.framework.ops import disable_eager_execution,enable_eager_execution\n",
    "from tensorflow.image import rgb_to_grayscale\n",
    "\n",
    "USE_TPU=False\n",
    "if 'COLAB_TPU_ADDR' not in os.environ:\n",
    "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
    "  !nvidia-smi\n",
    "else:\n",
    "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
    "#tf.config.experimental_connect_to_cluster(resolver)\n",
    "# This is the TPU initialization code that has to be at the beginning.\n",
    "#tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "    \n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2123,
     "status": "ok",
     "timestamp": 1668173703499,
     "user": {
      "displayName": "Maks Wojtas",
      "userId": "11103437561791788771"
     },
     "user_tz": 0
    },
    "id": "aHtLDO_HzpOC",
    "outputId": "b10092b5-ddf3-4a5a-b95d-eaccc30b16a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#@title Mount Drive\n",
    "from google.colab import drive\n",
    "if USE_TPU==True:\n",
    "  DRIVE_DIR = '/content/drive/MyDrive'\n",
    "else:\n",
    "  DRIVE_DIR = '/content/drive/My Drive'\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1668173707391,
     "user": {
      "displayName": "Maks Wojtas",
      "userId": "11103437561791788771"
     },
     "user_tz": 0
    },
    "id": "YgmzVB9Eb8j-",
    "outputId": "b1e78540-3fb2-4da6-94c7-9cacb3c7ec8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1026 backgrounds.\n"
     ]
    }
   ],
   "source": [
    "#if dataset_type == \"kbmg\" or dataset_type == \"shoes\" or dataset_type == \"wedding\" :\n",
    "BG_PATHS = np.array([os.path.join(dp, f) for dp, dn, fn in os.walk(\"BGs\") for f in fn])\n",
    "print(\"Found\", len(BG_PATHS),\"backgrounds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1668173708371,
     "user": {
      "displayName": "Maks Wojtas",
      "userId": "11103437561791788771"
     },
     "user_tz": 0
    },
    "id": "FhTgEL3ObxxY"
   },
   "outputs": [],
   "source": [
    "cats_dict = [\n",
    "        {'id': 0, 'name': 'dresses',   'nn_output_id': 0, 'feed_cats': [331, 332, 333, 334, 335, 336, 330]},\n",
    "        {'id': 1, 'name': 'tops',      'nn_output_id': 1, 'feed_cats': [241, 242, 243, 240, 291, 290, 341, 342, 343, 344, 340, 351, 352, 353, 350]},\n",
    "        {'id': 2, 'name': 'skirts',    'nn_output_id': 2, 'feed_cats': [321, 322, 323, 324, 325, 326, 327, 328, 320]},\n",
    "        {'id': 3, 'name': 'outerwear', 'nn_output_id': 3, 'feed_cats': [271, 272, 273, 274, 275, 276, 277, 278, 270, 281, 282, 283, 284, 280, 301, 302, 300]},\n",
    "        {'id': 4, 'name': 'leggings',  'nn_output_id': 4, 'feed_cats': [311, 312, 313, 314, 315, 316, 310]},\n",
    "        {'id': 5, 'name': 'footwear',  'nn_output_id': 0, 'feed_cats': [251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 250]},\n",
    "        {'id': 6, 'name': 'bags',      'nn_output_id': 0, 'feed_cats': [371, 372, 373, 374, 375, 376, 370]},\n",
    "]\n",
    "if dataset_type == \"kbmg\":\n",
    "  NOBG_TEST_FRAC =0.10\n",
    "  UNPROC_TEST_FRAC =0.05\n",
    "  N_CATEGORIES = 5\n",
    "elif dataset_type==\"wedding\":\n",
    "  NOBG_TEST_FRAC =0.15\n",
    "  UNPROC_TEST_FRAC =0.15\n",
    "  N_CATEGORIES = 5\n",
    "elif dataset_type in [\"shoes\", \"bags\", \"hats\"]:\n",
    "  NOBG_TEST_FRAC =0.15\n",
    "  UNPROC_TEST_FRAC =0.04\n",
    "  N_CATEGORIES = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1668174504591,
     "user": {
      "displayName": "Maks Wojtas",
      "userId": "11103437561791788771"
     },
     "user_tz": 0
    },
    "id": "eUXob9IXg0_1"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "N_ENCODING = 256\n",
    "N_NOISE = 256\n",
    "N_CAT_EMBEDDING = 16\n",
    "IMG_SHAPE=(300,300)\n",
    "INPUT_SHAPE=(300,300,3)\n",
    "BG_PROBABILITY = 0.0\n",
    "if dataset_type == \"kbmg\" or dataset_type==\"wedding\":\n",
    "  N_CATEGORIES = 5\n",
    "elif dataset_type==\"shoes\" or dataset_type==\"bags\":\n",
    "  N_CATEGORIES=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define discriminator, generator & WGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1668174505919,
     "user": {
      "displayName": "Maks Wojtas",
      "userId": "11103437561791788771"
     },
     "user_tz": 0
    },
    "id": "CpKC9nzW3--B",
    "outputId": "4353496e-630e-4fe8-8505-7ba8f98d02ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 300, 300, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 300, 300, 64  4864        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        multiple             0           ['conv2d[0][0]',                 \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 150, 150, 64  102464      ['leaky_re_lu[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 150, 150, 12  204928      ['leaky_re_lu[1][0]']            \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      multiple             0           ['conv2d_2[0][0]',               \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 75, 75, 128)  409728      ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 75, 75, 128)  0           ['leaky_re_lu_1[1][0]']          \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 75, 75, 128)  147584      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      multiple             0           ['conv2d_4[0][0]',               \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1, 16)        80          ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 38, 38, 128)  147584      ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 16)           0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 2888)         49096       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 38, 38, 128)  0           ['leaky_re_lu_2[1][0]']          \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 38, 38, 2)    0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 38, 38, 130)  0           ['dropout_1[0][0]',              \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 38, 38, 256)  299776      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      multiple             0           ['conv2d_6[0][0]',               \n",
      "                                                                  'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 19, 19, 256)  590080      ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 722)          185554      ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 19, 19, 256)  0           ['leaky_re_lu_3[1][0]']          \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 19, 19, 2)    0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 19, 19, 258)  0           ['dropout_2[0][0]',              \n",
      "                                                                  'reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 19, 19, 256)  594688      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      multiple             0           ['conv2d_8[0][0]',               \n",
      "                                                                  'conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 10, 10, 256)  590080      ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 10, 10, 256)  0           ['leaky_re_lu_4[1][0]']          \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 25600)        0           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 25600)        0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            25601       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,352,107\n",
      "Trainable params: 3,352,107\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#@title DISCRIMINATOR\n",
    "\n",
    "from tensorflow.keras.layers import Input, Concatenate, Reshape, Dense,\\\n",
    "                                    Embedding,GlobalAveragePooling2D,\\\n",
    "                                    BatchNormalization,Flatten,Dropout,\\\n",
    "                                    Conv2DTranspose, LeakyReLU, Conv2D\n",
    "from tensorflow.keras.models import Model   \n",
    "from tensorflow.keras import layers\n",
    "def conv_block(x, filters, activation, kernel_size=(3, 3),strides=(1, 1),\n",
    "              padding=\"same\",use_bias=True,use_bn=False,use_dropout=False,drop_value=0.5):\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=(1,1), padding=padding, \n",
    "                      use_bias=use_bias)(x)\n",
    "    x = activation(x)\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=strides, padding=padding, \n",
    "                      use_bias=use_bias)(x)\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x\n",
    "def get_discriminator(n_used_cats=0):\n",
    "    img_input = Input(INPUT_SHAPE)\n",
    "    cat_input = Input((1,),dtype=tf.dtypes.int32)\n",
    "    enc_input = Input((N_ENCODING,))    \n",
    "    embedded_category_input = Embedding(input_dim = n_used_cats,\n",
    "                                        output_dim = N_CAT_EMBEDDING,\n",
    "                                        input_length = 1)(cat_input)\n",
    "    embedded_category_input = Reshape((N_CAT_EMBEDDING,))(embedded_category_input)\n",
    "    embedded_category_input = Dense(38*38*2)(embedded_category_input)\n",
    "    embedded_category_input = Reshape((38,38,2))(embedded_category_input)\n",
    "    \n",
    "    embedded_encoding_input = Dense(19*19*2)(enc_input)\n",
    "    embedded_encoding_input = Reshape((19,19,2))(embedded_encoding_input)\n",
    "    \n",
    "\n",
    "    #x = Concatenate(axis=1)([backbone_output,enc_input,embedded_category_input])  \n",
    "    x = img_input\n",
    "    x = conv_block(x,64,kernel_size=(5, 5),strides=(2, 2),use_bn=False,\n",
    "                   use_bias=True,activation=layers.LeakyReLU(0.2),\n",
    "                   use_dropout=False,drop_value=0.3)  \n",
    "    x = conv_block(x,128,kernel_size=(5, 5),strides=(2, 2),use_bn=False,\n",
    "                   use_bias=True,activation=layers.LeakyReLU(0.2),\n",
    "                   use_dropout=True,drop_value=0.3)  \n",
    "    x = conv_block(x,128,kernel_size=(3, 3),strides=(2, 2),use_bn=False,\n",
    "                   use_bias=True,activation=layers.LeakyReLU(0.2),\n",
    "                   use_dropout=True,drop_value=0.3)  \n",
    "    x = Concatenate(axis=-1)([x,embedded_category_input])  \n",
    "\n",
    "    x = conv_block(x,256,kernel_size=(3, 3),strides=(2, 2),use_bn=False,\n",
    "                   use_bias=True,activation=layers.LeakyReLU(0.2),\n",
    "                   use_dropout=True,drop_value=0.3)  \n",
    "    x = Concatenate(axis=-1)([x,embedded_encoding_input])  \n",
    "    x = conv_block(x,256,kernel_size=(3, 3),strides=(2, 2),use_bn=False,\n",
    "                   use_bias=True,activation=layers.LeakyReLU(0.2),\n",
    "                   use_dropout=True,drop_value=0.3)  \n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs =[img_input,cat_input,enc_input] ,\n",
    "                  outputs=x, name=\"Discriminator\")\n",
    "    return model\n",
    "d_model = get_discriminator(N_CATEGORIES)\n",
    "d_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1030,
     "status": "ok",
     "timestamp": 1668174507867,
     "user": {
      "displayName": "Maks Wojtas",
      "userId": "11103437561791788771"
     },
     "user_tz": 0
    },
    "id": "MDx_Xvd8HvAh",
    "outputId": "6b445482-705e-4e0a-f980-ffffa54f7de0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1, 16)        80          ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 16)           0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 400)          6800        ['reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 800)          205600      ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 800)          205600      ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 400)         1600        ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 800)         3200        ['dense_3[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 800)         3200        ['dense_4[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 400)          0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 800)          0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 800)          0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)            (None, 5, 5, 16)     0           ['leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 5, 5, 32)     0           ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 5, 5, 32)     0           ['leaky_re_lu_6[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 5, 5, 80)     0           ['reshape_6[0][0]',              \n",
      "                                                                  'reshape_4[0][0]',              \n",
      "                                                                  'reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 10, 10, 80)   0           ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 10, 10, 256)  184320      ['up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 10, 10, 256)  0           ['conv2d_10[0][0]',              \n",
      "                                                                  'batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 10, 10, 256)  589824      ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 10, 10, 256)  1024       ['conv2d_11[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 30, 30, 256)  0          ['leaky_re_lu_8[1][0]']          \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 30, 30, 256)  589824      ['up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 30, 30, 256)  0           ['conv2d_12[0][0]',              \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 30, 30, 256)  589824      ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 30, 30, 256)  1024       ['conv2d_13[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 150, 150, 25  0          ['leaky_re_lu_9[1][0]']          \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 150, 150, 12  819200      ['up_sampling2d_2[0][0]']        \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)     (None, 150, 150, 12  0           ['conv2d_14[0][0]',              \n",
      "                                8)                                'batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 150, 150, 12  409600      ['leaky_re_lu_10[0][0]']         \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 150, 150, 12  512        ['conv2d_15[0][0]']              \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSampling2D)  (None, 300, 300, 12  0          ['leaky_re_lu_10[1][0]']         \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 300, 300, 3)  3456        ['up_sampling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 300, 300, 3)  0           ['conv2d_16[0][0]',              \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 300, 300, 3)  81          ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 300, 300, 3)  12         ['conv2d_17[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,614,781\n",
      "Trainable params: 3,609,495\n",
      "Non-trainable params: 5,286\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#@title GENERATOR\n",
    "def upsample_block(x,up_dim,\n",
    "    filters,activation,kernel_size=(3, 3),strides=(1, 1),\n",
    "    padding=\"same\",use_bn=False,use_bias=True,use_dropout=False,drop_value=0.3):\n",
    "    x = layers.UpSampling2D(up_dim)(x)\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=strides, padding=padding, \n",
    "                      use_bias=use_bias)(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=strides, padding=padding, \n",
    "                      use_bias=use_bias)(x)\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x\n",
    "def get_generator(n_used_cats=0):\n",
    "  noise_input = Input((N_NOISE,))\n",
    "  cat_input = Input((1,),dtype=tf.dtypes.int32)\n",
    "  enc_input = Input((N_ENCODING,))    \n",
    "  embedded_category_input = layers.Embedding(input_dim = n_used_cats,\n",
    "                                                    output_dim = N_CAT_EMBEDDING,\n",
    "                                                    input_length = 1)(cat_input)\n",
    "  embedded_category_input = Reshape((N_CAT_EMBEDDING,))(embedded_category_input)\n",
    "\n",
    "  side_size = 5\n",
    "  n_noise_filters = 32\n",
    "  n_cat_filters = 16\n",
    "  n_enc_filters = 32\n",
    "  n_units =  side_size*side_size\n",
    "\n",
    "  processed_noise = Dense(n_units*n_noise_filters)(noise_input)\n",
    "  processed_noise = BatchNormalization()(processed_noise)\n",
    "  processed_noise = LeakyReLU(0.5)(processed_noise)\n",
    "  processed_noise = Reshape((side_size,side_size,n_noise_filters))(processed_noise)\n",
    "\n",
    "  processed_enc = Dense(n_units*n_enc_filters)(enc_input)\n",
    "  processed_enc = BatchNormalization()(processed_enc)\n",
    "  processed_enc = LeakyReLU(0.5)(processed_enc)\n",
    "  processed_enc = Reshape((side_size,side_size,n_enc_filters))(processed_enc)\n",
    "\n",
    "  processed_cat = Dense(n_units*n_cat_filters)(embedded_category_input)\n",
    "  processed_cat = BatchNormalization()(processed_cat)\n",
    "  processed_cat = LeakyReLU(0.5)(processed_cat)\n",
    "  processed_cat = Reshape((side_size,side_size,n_cat_filters))(processed_cat)\n",
    "\n",
    "  x = Concatenate(axis=-1)([processed_cat,processed_noise,processed_enc])\n",
    "\n",
    "  x = upsample_block(x,(2,2),256,layers.LeakyReLU(0.2),kernel_size=(3,3),strides=(1, 1),\n",
    "        use_bias=False,use_bn=True,padding=\"same\",use_dropout=False,)\n",
    "  x = upsample_block(x,(3,3),256,layers.LeakyReLU(0.2),kernel_size=(3,3),strides=(1, 1),\n",
    "      use_bias=False,use_bn=True,padding=\"same\",use_dropout=False)\n",
    "  x = upsample_block(x,(5,5),128,layers.LeakyReLU(0.2),kernel_size=(5,5),strides=(1, 1),\n",
    "      use_bias=False,use_bn=True,padding=\"same\",use_dropout=False)\n",
    "  x = upsample_block(x,(2,2), 3, layers.Activation(\"tanh\"), strides=(1, 1), \n",
    "      use_bias=False, use_bn=True)\n",
    "\n",
    "  model = Model(inputs =[noise_input,cat_input,enc_input] ,\n",
    "                outputs=x, name=\"Generator\")\n",
    "  return model\n",
    "g_model = get_generator(N_CATEGORIES)\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1668174508271,
     "user": {
      "displayName": "Maks Wojtas",
      "userId": "11103437561791788771"
     },
     "user_tz": 0
    },
    "id": "dHPF3bFPoik7"
   },
   "outputs": [],
   "source": [
    "#@title wgan model\n",
    "\n",
    "class WGAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim,\n",
    "        discriminator_extra_steps=3,\n",
    "        gp_weight=10.0,\n",
    "    ):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer):#, d_loss_fn, g_loss_fn):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        #self.d_loss_fn = d_loss_fn\n",
    "        #self.g_loss_fn = g_loss_fn\n",
    "    #\"\"\"\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images,cats,encodings):\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator([interpolated,cats,encodings\n",
    "                                       \n",
    "                                       ], training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "    #\"\"\"\n",
    "    def train_step(self, x):\n",
    "        \n",
    "        real_images = x[0]\n",
    "        cats = x[1]\n",
    "        encodings = x[2]\n",
    "        rolled_cats = tf.roll(cats,1,axis=0)\n",
    "        rolled_encodings = tf.roll(encodings,1,axis=0)\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim),mean=0,stddev=1)\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_images = self.generator([random_latent_vectors,cats,encodings], training=True)\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator([fake_images,cats,encodings], training=True)\n",
    "                # Get the logits for the real images\n",
    "                real_logits = self.discriminator([real_images,cats,encodings], training=True)\n",
    "                # Get the logits for the real images, wrong encoding and category\n",
    "                rolled_logits = self.discriminator([real_images,rolled_cats,rolled_encodings], training=True)\n",
    "\n",
    "                untrue_logits = tf.concat([fake_logits,rolled_logits],axis=0)\n",
    "\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_images, fake_images,cats,encodings)\n",
    "                \n",
    "                d_loss = tf.reduce_mean(fake_logits) + tf.reduce_mean(untrue_logits) \\\n",
    "                        - 2*tf.reduce_mean(real_logits) + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "            if i==0:\n",
    "              saved_metrics = {\"fake_logits\":fake_logits,\"real_logits\":real_logits,\n",
    "                               \"rolled_logits\":rolled_logits,\"grad_pen\":gp}\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator([random_latent_vectors,cats,encodings], training=True)\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator([generated_images,cats,encodings], training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = -tf.reduce_mean(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        saved_metrics.update({\"d_loss\": d_loss, \"g_loss\": g_loss})\n",
    "        return saved_metrics\n",
    "# Instantiate the optimizer for both networks\n",
    "# (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
    "generator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "discriminator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "\n",
    "# Define the loss functions for the discriminator,\n",
    "# which should be (fake_loss - real_loss).\n",
    "# We will add the gradient penalty later to this loss function.\n",
    "def discriminator_loss(real_img, fake_img):\n",
    "    real_loss = tf.reduce_mean(real_img)\n",
    "    fake_loss = tf.reduce_mean(fake_img)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "# Define the loss functions for the generator.\n",
    "def generator_loss(fake_img):\n",
    "    return -tf.reduce_mean(fake_img)\n",
    "\n",
    "wgan = WGAN(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=N_NOISE,\n",
    "    discriminator_extra_steps=3,\n",
    ")\n",
    "generator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "discriminator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "wgan.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load previously trained triplet similarity model to provide encodings for WGAN conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1668174508680,
     "user": {
      "displayName": "Maks Wojtas",
      "userId": "11103437561791788771"
     },
     "user_tz": 0
    },
    "id": "WyC-eqhmGpnT"
   },
   "outputs": [],
   "source": [
    "#@title Get model fn\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, BatchNormalization,\\\n",
    "                                    Flatten, GlobalAveragePooling2D, Dropout\n",
    "\n",
    "tf_variable_VAE_loss_weight = tf.Variable(initial_value=0.0,trainable=False)\n",
    "\n",
    "def sample_z(args):\n",
    "  mu, sigma = args\n",
    "  batch     = K.shape(mu)[0]\n",
    "  dim       = K.int_shape(mu)[1]\n",
    "  eps       = K.random_normal(shape=(batch, dim))\n",
    "  return mu + K.exp(0.5*sigma) * eps\n",
    "\n",
    "def get_online_triplet_model(n_used_cats=0,version=0):\n",
    "    if(BACKBONE=='EfficientNetB1'):\n",
    "        from tensorflow.keras.applications.efficientnet import EfficientNetB1,preprocess_input\n",
    "        model_backbone = EfficientNetB1(weights='imagenet',include_top=False, input_shape=(244,244,3))\n",
    "    elif(BACKBONE=='VGG'):\n",
    "        from tensorflow.keras.applications.vgg16 import VGG16,preprocess_input\n",
    "        model_backbone = VGG16(weights='imagenet',include_top=False, input_shape=(244,244,3))\n",
    "    elif(BACKBONE=='EfficientNetB3'):\n",
    "        from tensorflow.keras.applications.efficientnet import EfficientNetB3,preprocess_input\n",
    "        model_backbone = EfficientNetB3(weights='imagenet',include_top=False, input_shape=INPUT_SHAPE)\n",
    "    elif(BACKBONE=='EfficientNetB4'):\n",
    "        from tensorflow.keras.applications.efficientnet import EfficientNetB4,preprocess_input\n",
    "        model_backbone = EfficientNetB4(weights='imagenet',include_top=False, input_shape=INPUT_SHAPE)\n",
    "    elif(BACKBONE=='xception'):\n",
    "        from tensorflow.keras.applications import Xception\n",
    "        from tensorflow.keras.applications.xception import preprocess_input\n",
    "        model_backbone = Xception(weights='imagenet',include_top=False, input_shape=INPUT_SHAPE)\n",
    "    model_backbone.trainable = False\n",
    "    x = GlobalAveragePooling2D(name=\"avg_pool\")(model_backbone.outputs[0])\n",
    "    x = BatchNormalization()(x)    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    if version == 9:\n",
    "      z_vae_encoding = Dense(1536,name=\"VAE_enc\")(x)\n",
    "      z_mean,z_logvar =tf.split(z_vae_encoding, num_or_size_splits=2, axis=1)            \n",
    "      x = layers.Lambda(sample_z, output_shape=(1536//2, ), name='sampled_enc')([z_mean, z_logvar])      \n",
    "    else:\n",
    "      x = Dense(1024,activation='relu')(x)\n",
    "      x = Dropout(0.3)(x)\n",
    "      inside_encodings=[]\n",
    "      inside_attention=[]\n",
    "      for i in range(n_used_cats):        \n",
    "          x_encoding = Dense(N_ENCODING)(x)\n",
    "          x_encoding = K.l2_normalize(x_encoding, axis=-1)\n",
    "          inside_encodings.append(x_encoding)\n",
    "      x_encoding = tf.stack(inside_encodings,axis=1)\n",
    "\n",
    "    preprocessing_fn = preprocess_input\n",
    "    inp = Input(INPUT_SHAPE)\n",
    "\n",
    "    one_model = Model(inputs = model_backbone.inputs,outputs=x_encoding, name=\"EncodingMod\")    \n",
    "    encodings = one_model(inp)\n",
    "    \n",
    "    inp_cats = Input((1,),dtype=tf.dtypes.int32)\n",
    "    inputs = [inp, inp_cats]\n",
    "    out_encodings = tf.gather_nd(encodings,indices=inp_cats,batch_dims=1,name=\"OutEncodings\")\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = out_encodings)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model, preprocessing_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator for WGAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1668174509705,
     "user": {
      "displayName": "Maks Wojtas",
      "userId": "11103437561791788771"
     },
     "user_tz": 0
    },
    "id": "dI9QqKduuMf2"
   },
   "outputs": [],
   "source": [
    "#@title wgan_data_gen\n",
    "from skimage.color import deltaE_ciede2000\n",
    "import gc\n",
    "                            \n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,preprocess_fn,batch_size, img_aug, shuffle=True):\n",
    "        'Initialization' \n",
    "        self.random_state= np.random.RandomState(seed=0)       \n",
    "        self.shuffle=shuffle\n",
    "        self.preprocess_fn=preprocess_fn\n",
    "        self.reset_state = True\n",
    "        self.batch_size = batch_size\n",
    "        self.on_epoch_end()\n",
    "        self.img_aug = img_aug\n",
    "        self.cache_used = False\n",
    "        print(\"Total length:\",len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil((N_PHOTOS) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):        \n",
    "        'Generate one batch of data'        \n",
    "        gc.collect()\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        bs = len(indexes)\n",
    "        X_img = np.empty((bs,)+INPUT_SHAPE)\n",
    "        X_cats = np.empty((bs,)+(1,))\n",
    "        X_encs = self.encodings[indexes]\n",
    "        if self.cache_used is True:\n",
    "          X_img = self.preprocess_fn(self.img_cache[indexes])\n",
    "          X_cats = np.array(merged_data[indexes,1],dtype='float')\n",
    "        else:\n",
    "          for i, idx in enumerate(indexes):          \n",
    "            X_img[i],X_cats[i] = self.get_img(idx)\n",
    "\n",
    "          if self.img_aug is True:\n",
    "            X_img = next(self.image_aug.flow(X_img,shuffle=False,batch_size=bs))\n",
    "          X_img = self.preprocess_fn(X_img)\n",
    "        #print(X_img.min(),X_img.max(),X_img.mean())\n",
    "        return [X_img, X_cats, X_encs]\n",
    "\n",
    "    def calculate_encodings(self,checkpoint,version,n_used_cats):\n",
    "      triplet_model, preprocessing_fn = get_online_triplet_model(n_used_cats=n_used_cats,version=version)\n",
    "      triplet_model.get_layer('EncodingMod').load_weights(checkpoint)\n",
    "      #triplet_model.load_weights(checkpoint)\n",
    "      def data_gen():\n",
    "        all_indexes = np.arange(len(self.indexes))\n",
    "        for i in range(self.__len__()):\n",
    "          start_idx = i*self.batch_size\n",
    "          end_idx = start_idx+self.batch_size\n",
    "          chosen_indexes = all_indexes[start_idx:end_idx]#self.indexes[start_idx:end_idx]\n",
    "          bs = len(chosen_indexes)\n",
    "          X_img = np.empty((bs,)+INPUT_SHAPE)\n",
    "          X_cats = np.empty((bs,)+(1,))\n",
    "          for i, idx in enumerate(chosen_indexes):          \n",
    "            X_img[i],X_cats[i] = self.get_img(idx)\n",
    "          X_img = preprocessing_fn(X_img)\n",
    "          yield ([X_img,X_cats],None)\n",
    "      #self.encodings = np.empty((len(self.indexes),N_ENCODING))\n",
    "      self.encodings = triplet_model.predict(x=data_gen(),steps=self.__len__(),verbose=1)\n",
    "      del triplet_model\n",
    "      K.clear_session()\n",
    "      return\n",
    "\n",
    "    def load_images(self):\n",
    "      print(\"Predicted size:\",len(self.indexes)*INPUT_SHAPE[0]*INPUT_SHAPE[1]*3/((2**10)**3))\n",
    "      assert len(self.indexes)*INPUT_SHAPE[0]*INPUT_SHAPE[1]*3<15*((2**10)**3) #smaller than 15 GB\n",
    "      self.cache_used = True\n",
    "      self.img_cache = np.empty((len(self.indexes),INPUT_SHAPE[0],INPUT_SHAPE[1],3),dtype=np.uint8)   \n",
    "      print(self.img_cache.nbytes)   \n",
    "      for idx in np.arange(len(self.indexes)):   \n",
    "        if idx%100==0:\n",
    "          gc.collect() \n",
    "        img,_ = self.get_img(idx)    \n",
    "        self.img_cache[idx] = img\n",
    "      return        \n",
    "\n",
    "    def get_img(self,img_idx):\n",
    "      img_mode = \"rgba\" if merged_data[img_idx,3]==1 else \"rgb\"\n",
    "      if os.path.isfile(merged_data[img_idx,0])==False:\n",
    "        raise ValueError(\"Image not present at path: \"+str(merged_data[img_idx,0]),img_idx)\n",
    "      img = load_img(merged_data[img_idx,0],color_mode=img_mode)\n",
    "      img = img_to_array(img)\n",
    "      if merged_data[img_idx,3]==1 and rs.binomial(1,BG_PROBABILITY)==1:\n",
    "        bg = img_to_array(load_img(BG_PATHS[rs.randint(0,len(BG_PATHS),1)[0]]))\n",
    "        img = overlay_random(bg, img ,rs)      \n",
    "      else:\n",
    "        img = img[:,:,:3]        \n",
    "      return img, merged_data[img_idx,1]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(N_PHOTOS)         \n",
    "        if(self.shuffle==True)      :\n",
    "          perm = self.random_state.permutation(len(self.indexes))\n",
    "          self.indexes = self.indexes[perm]   \n",
    "\n",
    "    def perform_visual_check(self,check_idx,checkpoint,version,n_used_cats):      \n",
    "      \n",
    "      triplet_model, preprocessing_fn = get_online_triplet_model(n_used_cats=n_used_cats,version=version)\n",
    "      triplet_model.load_weights(checkpoint)\n",
    "      img,cat = self.get_img(check_idx)\n",
    "      preprocessed_img = preprocessing_fn(img[None,...])\n",
    "      enc_pred = triplet_model.predict([preprocessed_img,np.array(cat,dtype='float')[None,...]])\n",
    "      if self.cache_used is True:\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(self.img_cache[check_idx])          \n",
    "        plt.subplot(1,2,2)\n",
    "      plt.imshow(img/255.0)        \n",
    "      plt.show()\n",
    "      print(\"Cat:\",cat)\n",
    "      print(\"Predicted enc:\",enc_pred[0,:6])\n",
    "      print(\"True enc:\",self.encodings[check_idx][:6])    \n",
    "      del triplet_model\n",
    "      K.clear_session()\n",
    "\n",
    "def discr_preprocess_fn(x):\n",
    "  x = x/255.0\n",
    "  return x+tf.random.normal(x.shape,mean=0,stddev=2.0/255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precalculate encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 147418,
     "status": "ok",
     "timestamp": 1668174662446,
     "user": {
      "displayName": "Maks Wojtas",
      "userId": "11103437561791788771"
     },
     "user_tz": 0
    },
    "id": "nGKJdNt4-qtQ",
    "outputId": "51f2b3e0-b6b7-448b-8261-8d56a52c0b9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 672\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 300, 300, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " EncodingMod (Functional)       (None, 5, 256)       13675567    ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_nd (TFOpLa  (None, 256)         0           ['EncodingMod[0][0]',            \n",
      " mbda)                                                            'input_9[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,675,567\n",
      "Trainable params: 2,888,960\n",
      "Non-trainable params: 10,786,607\n",
      "__________________________________________________________________________________________________\n",
      "672/672 [==============================] - 141s 206ms/step\n"
     ]
    }
   ],
   "source": [
    "BACKBONE='EfficientNetB3'\n",
    "data_gen = DataGenerator(preprocess_fn=discr_preprocess_fn,batch_size=BATCH_SIZE, img_aug=False, shuffle=True)\n",
    "data_gen.calculate_encodings(os.path.join(DRIVE_DIR,\"checkpoints\",\"s2s_v6_EfficientNetB3_300_256_03_KbmgV3C.h5\"),6,n_used_cats=N_CATEGORIES)\n",
    "assert len(data_gen.encodings) == len(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform visual check to check validity of encodings using TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYMS-HdPm0Tx"
   },
   "outputs": [],
   "source": [
    "data_gen.load_images()\n",
    "data_gen.perform_visual_check(10000,os.path.join(DRIVE_DIR,\"checkpoints\",\"s2s_v6_EfficientNetB3_300_256_03_KbmgBothAugReviewedUnfrzH.ckpt\"),6,n_used_cats=N_CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 23588,
     "status": "ok",
     "timestamp": 1668175316732,
     "user": {
      "displayName": "Maks Wojtas",
      "userId": "11103437561791788771"
     },
     "user_tz": 0
    },
    "id": "hbNGNWOF2n-6"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "no_bg_idx = [i for i in range(len(data_gen.encodings)) if  merged_data[i,1]=='0'] #merged_data[i,3]=='1' and merged_data[i,1]=='1']\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random').fit_transform(data_gen.encodings[no_bg_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 235754,
     "status": "ok",
     "timestamp": 1668175552483,
     "user": {
      "displayName": "Maks Wojtas",
      "userId": "11103437561791788771"
     },
     "user_tz": 0
    },
    "id": "rl6ZEo7U8AbT",
    "outputId": "2bb42db1-3c98-44be-e63d-6182aac90921"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(100,100))\n",
    "ax.scatter(X_embedded[:,0], X_embedded[:,1],s=0) \n",
    "random_indexes = np.random.choice(np.arange(len(X_embedded)),size=3000,replace=False)\n",
    "\n",
    "def getImage(path):\n",
    "    return OffsetImage(plt.imread(path))\n",
    "\n",
    "for x0, y0, path in zip(X_embedded[random_indexes,0], X_embedded[random_indexes,1],(merged_data[no_bg_idx,0])[random_indexes]):\n",
    "    ab = AnnotationBbox(getImage(path), (x0, y0), frameon=False)\n",
    "    ax.add_artist(ab)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(os.path.join(DRIVE_DIR,\"dresses.png\"))\n",
    "plt.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare wGAN callbacks and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "skoaOINioNU6"
   },
   "outputs": [],
   "source": [
    "#@title GAN plotter to check results\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img, data_gen):\n",
    "        self.num_img = num_img\n",
    "        self.img_indexes = np.random.choice(N_PHOTOS,size=num_img,replace=False)        \n",
    "        self.latent_vec = tf.random.normal(shape=(self.num_img, N_NOISE))\n",
    "        self.encodings = data_gen.encodings[self.img_indexes]\n",
    "        self.img_path = os.path.join(DRIVE_DIR,\"wgan_images\")\n",
    "        base_imgs = []\n",
    "        cats=[]\n",
    "        for i in range(num_img):\n",
    "          img,cat = data_gen.get_img(self.img_indexes[i])\n",
    "          base_imgs.append(img)\n",
    "          cats.append(cat)\n",
    "        self.cats = np.array(cats,dtype='int')\n",
    "        tiled_img = np.concatenate(base_imgs,axis=1)\n",
    "        tiled_img = keras.preprocessing.image.array_to_img(tiled_img)\n",
    "        tiled_img.save(os.path.join(self.img_path,\"generated_base_img.png\"))\n",
    "        \n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        generated_images = self.model.generator.predict([self.latent_vec,self.cats,self.encodings])\n",
    "        generated_images = (generated_images * 127.5) + 127.5\n",
    "        #generated_images = generated_images.numpy()\n",
    "        tiled_img = np.concatenate(generated_images,axis=1)\n",
    "        tiled_img = keras.preprocessing.image.array_to_img(tiled_img)\n",
    "        tiled_img.save(os.path.join(self.img_path,\"generated_{epoch}_img.png\").format(epoch=epoch+1))\n",
    "\n",
    "monitor_cb = GANMonitor(10,data_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ew7iqSZ_wIyq"
   },
   "outputs": [],
   "source": [
    "data_gen.batch_size=BATCH_SIZE\n",
    "data_gen.on_epoch_end()\n",
    "monitor_cb.set_model(wgan)\n",
    "monitor_cb.on_epoch_end(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 34421,
     "status": "error",
     "timestamp": 1638895625444,
     "user": {
      "displayName": "Maks Wojtas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiTCyEeMsETGANb159xHh6rPT0tjQEvbsrDmbW_Tg=s64",
      "userId": "11103437561791788771"
     },
     "user_tz": 0
    },
    "id": "ieAtG8n5tzb1",
    "outputId": "1dd3b05f-9ae7-4c59-8e3e-fd756969d634"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "strategy = tf.distribute.TPUStrategy(resolver)\n",
    "\n",
    "wgan.fit(data_gen,callbacks=[tensorboard_cb,monitor_cb],steps_per_epoch=100)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO4dfVPSkewnghU40wOyxkM",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1IvBymsit901H8B7-nV62T1NE6ldMF4-W",
     "timestamp": 1635422560599
    },
    {
     "file_id": "1ApZnVpVUNRxZq5hpQ1QwMPKDJf5U1aVi",
     "timestamp": 1608027787705
    },
    {
     "file_id": "1l27uLSOXaPsWcXzv_fk0PbjyjokTekNH",
     "timestamp": 1603361964160
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
